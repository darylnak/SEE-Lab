{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Reading the data\n",
    "\n",
    "The data for this project comes from the \"Physionet\" database collected by the Children's Hospital of Boston. The data consits of EEG recordings gathered from 10 children and young adults with medically resistant epilepsy. Each test subject has around 50 hours of recordings so there's a lot of data! A neurologist then went through all the data by hand and identified all the seizures. So each sample gets a value of `1` if it corresponds to a seizure and a `0` if not. The data was collected in a hospital setting and more subjects have data from at least 20 different electrodes. However, to be closer to the environment faced by developers of embedded systems we'll only look at two channels that are located near the temples. You can see some more detailed information about the data [here](https://physionet.org/content/chbmit/1.0.0/).\n",
    "\n",
    "In the `../input/` folder you'll find a file `eeg_data_temples2.h5` which contains all the EEG recordings for the two channels we're interested in. Our first step will be to read this data into Python and convert it into a format our algorithm can work with. To do so, we'll use the `tables` library for Python which can read the `HDF5` file format which is used to store the EEG data.\n",
    "\n",
    "## Side Note: Model Estimation and Feature Extraction\n",
    "\n",
    "For this task we want to classify a short segment (commonly called a \"window\") of an EEG recording (4 seconds to be exact) as either being a during a seizure or not. To do so, we will use a logistic regression model. Very succinctly, a lostic regression is a statistical model which can be used predicts the value of a binary random variable (i.e. a random variable which takes on exactly two discrete values) known as the \"outcome\" or \"dependent variable,\" given a set of observations of one or more \"input\" or \"independent\" variables. Very succinctly, suppose we have two input variables $x_1 \\in \\mathbb{R}$ and $x_2 \\in \\mathbb{R}$ (where $\\mathbb{R}$ denotes the real numbers) and a discrete outcome $y \\in \\{0,1\\}$. Then logistic regression first computes a linear function of those variables:\n",
    "\n",
    "$$ z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 $$\n",
    "\n",
    "and then \"squashes\" $z$ through a function (called the logistic-sigmoid) which converts $z$ into a probability. Putting both steps together we have:\n",
    "\n",
    "$$ Pr(y = 1 | x_1, x_2) = \\sigma(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2) $$\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1+e^{-x}}$ is the logistic sigmoid. In general, we don't know $\\beta_0, \\beta_1, \\beta_2$ and need to *estimate* them from some data. The $\\beta$ are known as the model \"parameters\" and the process of estimating them is called \"parameter estimation\" (there's an entire ECE course that is just about parameter estimation). The algorithm to do this is somewhat complicated, but fortunately some other people have already implemented it for us. We just need to give their implementation a bunch of data corresponding to several observed values of $x_1$ and $x_2$ along with the correct value of $y$ for those data points. Their algorithm will give us back the \"best\" values of $\\beta$. Then, if we observe values for $x_1$ and $x_2$ for which we *do not* know the correct value of $y$, we can plug those into the equation above along with the estimated values for the $\\beta$ and compute the probabilty that these $x$ correspond to $y=1$. \n",
    "\n",
    "This is a very over-simplified description of logistic regression which has a number of interesting interpretations both statistically and geometrically which you're encouraged to read more about [here](https://en.wikipedia.org/wiki/Logistic_regression). In particular, I recommend reading the first two \"examples\" to get a bit more technical understanding of how the model works and how we define the \"best\" value of the $\\beta$.\n",
    "\n",
    "In our case the outcome variable will be \"given the last four seconds of EEG data, is the person having a seizure?\" So in this case our $x$ variables will be some variables which summarize the last four seconds of observed EEG data and our $y$ variable will be a $1$ if the person was having a seizure at any point in the last four seconds. The question of how to best summarize the signal so that a logistic regression gives good results has been the subject of considerable research and people have proposed a bunch of different values to extract from the seizure which may be useful. In this case we'll use a very simple model that uses the mean and variance of the signal in each window along with some features of the \"frequency domain\" which are well known to be useful by medical researchers. The term \"frequency domain\" refers to the representation of the signal obtained by computing its Fourier Transform. Recall that the [Fourier Transform](https://en.wikipedia.org/wiki/Fourier_transform) takes a signal which is measured as a function of time and decomposes it into a superposition of primitive waves oscilating at different frequencies. In many cases (including this one), the relative importance of these waves tells us useful things about the signal so it's very common to analyze signals in the frequency domain. If you don't know about Fourier Transforms, that's fine, but I recommend reading about them (or look up the 3Blue1Brown video about them) because they are very interesting.\n",
    "\n",
    "So our first task will be to read the raw data collected on EEG and extract the various \"features\" from the raw signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables\n",
    "import logging # for debug logging\n",
    "from tqdm import tqdm # to display a loading bar for iterables\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import signal\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "import sklearn as sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "import os.path # to find and create directories\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(X):\n",
    "    N = len(X)\n",
    "    K = np.ones((N,N))\n",
    "    for ixi in tqdm.tqdm(range(N)):\n",
    "        P = X[ixi]\n",
    "        for ixj in range(ixi+1,N):\n",
    "            Q = X[ixj]\n",
    "            v = min(P.shape[1], Q.shape[1])\n",
    "            K[ixi,ixj] = (1/float(v))*np.square(np.linalg.norm(P.T.dot(Q)))\n",
    "\n",
    "    ixs = np.tril_indices(N)\n",
    "    K[ixs] = K.T[ixs]\n",
    "    return K\n",
    "\n",
    "# downsample training data to balance number of non-seizure and seizure data points (50/50 in this case)\n",
    "def downSample(data_train, label_train):\n",
    "\n",
    "    # since labels are bools, number of Trues is the number of seizures\n",
    "    numSeizures = np.sum(label_train)\n",
    "    \n",
    "    # mask to select all non-seizure data for subject\n",
    "    isFalse = (label_train == False)\n",
    "    \n",
    "    # select all non-seizure data for subject\n",
    "    data_train_false = data_train[isFalse, :]\n",
    "\n",
    "    # randomly sample numSeizures rows from non-seizure data\n",
    "    sample = np.random.randint(np.size(data_train_false, 0), size = numSeizures)\n",
    "    data_train_false = data_train[sample, :]\n",
    "    \n",
    "    if debug:\n",
    "        print('Number of seizures {}'.format(numSeizures))\n",
    "        print('Shape of false data after sample {}'.format(data_train_false.shape))\n",
    "    \n",
    "    # create same amount of zeroes (False) as number of seizures\n",
    "    label_false = np.zeros(numSeizures, dtype='bool')\n",
    "\n",
    "    if debug:\n",
    "        print(label_false)\n",
    "    \n",
    "    # capture all data marked as seizure\n",
    "    data_train_true = data_train[np.logical_not(isFalse), :]\n",
    "    label_true = np.ones(numSeizures, dtype='bool')\n",
    "                                  \n",
    "    if debug:\n",
    "        print(label_true)\n",
    "    \n",
    "    data_train_down = np.concatenate((data_train_false, data_train_true))\n",
    "    label_train_down = np.concatenate((label_false, label_true))\n",
    "\n",
    "    if debug:\n",
    "        print('Shape of false data labels after sample {}'.format(label_train_down.shape))\n",
    "        print(label_train_down)\n",
    "    \n",
    "    return data_train_down, label_train_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the file. It contains several \"nodes\" each of which corresponds to one test subject\n",
    "# Each node has several children which each correspond to (roughly) one hour of data.\n",
    "\n",
    "# h5_file = tables.open_file(\"../input/eeg_data_temples2.h5\")\n",
    "# print(h5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function computes the relative power in several frequency bands which\n",
    "# are generally known to be medically relevant\n",
    "\n",
    "def compute_band_relpower(X):\n",
    "    band_relpower = []\n",
    "    bands = [(0.5,4), (4,8), (8,13), (13,32), (32,60)]\n",
    "\n",
    "    freqs, psd = signal.welch(X, fs=256.0, axis=0)\n",
    "    fr_res = freqs[1] - freqs[0]\n",
    "\n",
    "    where = lambda lb, ub: np.logical_and(freqs >= lb, freqs < ub)\n",
    "    abs_power = np.concatenate(\n",
    "            [integrate.simps(psd[where(lb, ub),:], dx=fr_res, axis=0\n",
    "                ).reshape(1,-1) for lb, ub in bands]\n",
    "        )\n",
    "    total_power = integrate.simps(psd, dx=fr_res, axis=0).astype(np.float64)\n",
    "    abs_power[:,total_power == 0] = 0\n",
    "    total_power[total_power == 0] = -1\n",
    "    band_relpower = (abs_power / total_power).ravel()    \n",
    "\n",
    "    return band_relpower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll read the data into a Python data structure and \n",
    "# extract features of the signal to be used in a statistical \n",
    "# algorithm for prediction\n",
    "def readData():\n",
    "    \n",
    "    # try to load data from files\n",
    "    try:\n",
    "        print('Loading features.npy...')\n",
    "        features = np.load('data/features.npy')\n",
    "\n",
    "        print('Loading labels.npy...')\n",
    "        labels = np.load('data/labels.npy')\n",
    "\n",
    "        print('Loading subjects.npy...')\n",
    "        subjects = np.load('data/subjects.npy')\n",
    "\n",
    "        print('All data was succesfully loaded!\\n')\n",
    "        return features, labels, subjects\n",
    "\n",
    "    # rebuild data if a file is missing\n",
    "    except IOError:\n",
    "        print('File was not found. Rebuilding data...')\n",
    "\n",
    "    # open file\n",
    "    h5_file = tables.open_file(\"../input/eeg_data_temples2.h5\")\n",
    "\n",
    "    directory = './data/'\n",
    "    \n",
    "    sampling_rate = 256     # how many observations (samples) are gathered every? second\n",
    "    window_size_seconds = 4 # how many seconds of data do we want in a window?\n",
    "    stride_seconds = 2      # how far should we advance the window at each step?\n",
    "\n",
    "    window_size = window_size_seconds*sampling_rate\n",
    "    stride = stride_seconds*sampling_rate\n",
    "\n",
    "    features = []\n",
    "    labels   = []\n",
    "    subjects = []\n",
    "    for node in h5_file.walk_nodes(\"/\", \"CArray\"):\n",
    "        node_data = node.read()\n",
    "        subject_id = node._v_name.split(\"/\")[0].split(\"_\")[0]\n",
    "\n",
    "        # The data for each node is an N x 3 numpy array (a matrix)\n",
    "        # The first two columns are the EEG data and the third is an\n",
    "        # indicator variable which is equal to 1 if the observation \n",
    "        # corresponds to a seizure\n",
    "\n",
    "        X, y = node_data[:,:-1], node_data[:,-1]\n",
    "        num_obs, num_channels = X.shape\n",
    "\n",
    "        # Now we want to convert our data matrix (X) into a sequence\n",
    "        # of overlapping windows\n",
    "\n",
    "        for ix in range(window_size, num_obs, stride):\n",
    "            X_w = X[ix-window_size:ix,:]\n",
    "\n",
    "            # now let's extract the \"features\" mentioned above\n",
    "            # TODO: compute the mean and variance of both channels of data\n",
    "            # hint: use np.mean and np.var and look up the \"axis\" argument\n",
    "            band_relpower = compute_band_relpower(X_w)\n",
    "            means = np.mean(X_w, axis = 0)    # compute mean for each col\n",
    "            variances = np.var(X_w, axis = 0) # compute var for each col\n",
    "            feature_vector = np.concatenate((band_relpower, means, variances))\n",
    "            features.append(feature_vector)\n",
    "\n",
    "            labels.append(np.any(y[ix-window_size:ix]))\n",
    "            subjects.append(subject_id)\n",
    "        \n",
    "    features = np.vstack(features) # stack each tuple in list on top of each other\n",
    "    labels = np.array(labels)      # converting these lists to numpy arrays\n",
    "    subjects = np.array(subjects)\n",
    "    \n",
    "    # make directory if it dosent exist\n",
    "    if not os.path.isdir(directory):\n",
    "        os.mkdir(directory)\n",
    "\n",
    "    # save arrays to respective files so we don't need to do this again\n",
    "    np.save('data/features', features)\n",
    "    np.save('data/labels', labels)\n",
    "    np.save('data/subjects', subjects)\n",
    "    \n",
    "    # close file\n",
    "    h5_file.close()\n",
    "\n",
    "    return features, labels, subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainAndTest(subID, data, label, subjects):\n",
    "\n",
    "    is_test_set = (subjects != subID) # mask to select data for everyone, but subID\n",
    "\n",
    "    data_train = data[is_test_set, :] # get entire row of subject in data\n",
    "    label_train = label[is_test_set] # get label of row\n",
    "    data_train, label_train = downSample(data_train, label_train)\n",
    "\n",
    "    if debug:\n",
    "        print('getTrainAndTest - data {}, label {}'.format(data_train.shape, label_train.shape))\n",
    "\n",
    "    # TODO: use the \"is_test_set\" variable to extract the rows of `X`\n",
    "    # corresponding to the test subject.\n",
    "    data_test = data[np.invert(is_test_set), :]\n",
    "    label_test = label[np.invert(is_test_set)]\n",
    "    \n",
    "    # As a final step, we'll resacle each of our features so that they\n",
    "    # have a mean of zero and a standard deviation of one. Again, the \n",
    "    # reason why we do this is a bit technical, but in general ML models\n",
    "    # don't like it when the features are on different scales.\n",
    "\n",
    "    S = StandardScaler()\n",
    "    data_train = S.fit_transform(data_train)\n",
    "    data_test = S.transform(data_test)\n",
    "\n",
    "    if debug:\n",
    "        print('getTrainAndTest 2- data {}, label {}'.format(data_train.shape, label_train.shape))\n",
    "    \n",
    "    return data_train, label_train, data_test, label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: construct a logistic regression object and call it's .fit()\n",
    "# method with the training data\n",
    "def trainModel(data_train, label_train, data_test, label_test, subID):\n",
    "\n",
    "    directory = 'probs/'\n",
    "    filename = '{}model'.format(subID)\n",
    "    file_path = directory + filename\n",
    "\n",
    "    if debug:\n",
    "        print(label_train[0])\n",
    "        print(label_train[-1])\n",
    "        \n",
    "    clf = LogisticRegression(random_state=0)\n",
    "    clf.fit(data_train, label_train)\n",
    "\n",
    "    # Now let's evaluate the performance of our model on the train and\n",
    "    # test set. We can use the \"predict\" method of the model to obtain\n",
    "    # predictions and compare these against the correct values\n",
    "    # TODO: compute the accuracy of the model on the training and testing\n",
    "    # datasets\n",
    "\n",
    "    label_pred_train = clf.predict(data_train)\n",
    "    train_accuracy = np.mean(np.equal(label_pred_train, label_train))\n",
    "\n",
    "    label_pred_test = clf.predict(data_test)\n",
    "    test_accuracy = np.mean(np.equal(label_pred_test, label_test))\n",
    "\n",
    "    if not os.path.isdir(directory):\n",
    "        os.mkdir(directory)\n",
    "\n",
    "    # save best model so we can analyze probabilities of best fit model later\n",
    "    probs = clf.predict_proba(data_test)\n",
    "    np.save(file_path, probs)\n",
    "\n",
    "    return label_pred_train, label_pred_test\n",
    "    #print(\"Training accuracy was: {}\".format(train_accuracy))\n",
    "    #print(\"Test accuracy was: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute the \"confusion matrix\" for the model\n",
    "# (hint: use the \"confusion_matrix\" function) and then use this\n",
    "# to compute the true negative rate and true positive rate\n",
    "def computeTrueNegAndPos(label_pred_test, label_test):\n",
    "    \n",
    "    cfn = confusion_matrix(label_pred_test, label_test)\n",
    "    \n",
    "    # number of correctly classified 0's divided by number of 0's in test\n",
    "    true_negatives = cfn[0][0] / (len(label_test) - sum(label_test)) \n",
    "    \n",
    "    # number of correctly classified 1's divided by number of 1's in test\n",
    "    true_positives = cfn[1][1] / sum(label_test)\n",
    "    \n",
    "    return true_negatives, true_positives\n",
    "    #print(\"True negative rate: {}\".format(true_negatives))\n",
    "    #print(\"True positive rate: {}\".format(true_positives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.mean(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: draw a random sample of the negative (0) training examples\n",
    "# to ensure there are an equal number of 0's and 1's in the training\n",
    "# data and estimate the model again.\n",
    "\n",
    "# print(\"Training accuracy was: {}\".format(train_accuracy))\n",
    "# print(\"Test accuracy was: {}\".format(test_accuracy))\n",
    "\n",
    "# print(\"True negative rate: {}\".format(true_negatives))\n",
    "# print(\"True positive rate: {}\".format(true_positives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModelOnSubject(subID, features, labels, subjects):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Train model on data, excluding subID. Then, test the model using subID as\n",
    "        the validation set.\n",
    "    \n",
    "    Input:\n",
    "        String subID - ID of the subject we exclude from training and use for validation.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    data_train, label_train, data_test, label_test = getTrainAndTest(subID, features, labels, subjects)\n",
    "    label_pred_train, label_pred_test = trainModel(data_train, label_train, data_test, label_test, subID)\n",
    "    \n",
    "    # only computing for test set\n",
    "    true_negatives, true_positives = computeTrueNegAndPos(label_pred_test, label_test)\n",
    "    \n",
    "    return true_negatives, true_positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MMD\n",
    "\n",
    "\\begin{equation}\n",
    "    ||X - Q_W||_{2}^{2} = \\frac{1}{M^2}\\sum_{i,j = 1}^{M}k(x_i, x_j)-\\frac{2}{MW}\\sum_{i=1}^{M}\\sum_{j=1}^{W}k(x_i,q_j)+\\frac{1}{W^2}\\sum_{i,j=1}^{W}k(q_i, q_j)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(Y,X):\n",
    "    result = sklearn.metrics.pairwise.rbf_kernel(X, Y)\n",
    "    return result\n",
    "\n",
    "def mmd(subID, data, labels, subjects, kernel='gaussian'):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        subID - ID of subject used in validation\n",
    "        features - n x m training data (n training points, m features)\n",
    "        labels - n corresponding labels for each training point\n",
    "        subjects - corresponding subject ID's for each training point\n",
    "        kernel - name of the kernel function used in MMD. By default, uses gaussin\n",
    "        \n",
    "        (1044454, 14)\n",
    "        (1044454,)\n",
    "        (1044454,)\n",
    "        \n",
    "    Returns:\n",
    "        true_negative - true negative value\n",
    "        true_positive - true positive value\n",
    "    \"\"\"\n",
    "    \n",
    "    SEIZURE = 1\n",
    "    WIN_SIZE = 100\n",
    "    BATCH_SIZE = 100\n",
    "    \n",
    "    preds = []\n",
    "    truth = []\n",
    "    \n",
    "    seiz_score  = 0\n",
    "    nseiz_score = 0\n",
    "    \n",
    "    # mask for training data\n",
    "    is_test_set  = (subjects != subID)\n",
    "    \n",
    "    train_data   = data[is_test_set, :]\n",
    "    train_labels = labels[is_test_set]\n",
    "    \n",
    "    sub_data     = data[np.invert(is_test_set), :]\n",
    "    sub_labels   = labels[np.invert(is_test_set)]\n",
    "    \n",
    "    is_seizure   = (train_labels == SEIZURE)\n",
    "    \n",
    "    seizures = train_data[is_seizure, :]\n",
    "    non_seizures = train_data[np.invert(is_seizure), :]    \n",
    "    \n",
    "    assert(len(train_data) + len(sub_data) == len(data))\n",
    "    assert(len(seizures) + len(non_seizures) == len(train_data))\n",
    "  \n",
    "\n",
    "    K = kernel(Z)\n",
    "    mmd = []\n",
    "    for N in range(1,Z.shape[0]):\n",
    "        M = Z.shape[0] - N\n",
    "        Kxx = K[:N,:N].sum()\n",
    "        Kxy = K[:N,N:].sum()\n",
    "        Kyy = K[N:,N:].sum()\n",
    "        mmd.append(np.sqrt(\n",
    "            ((1/float(N*N))*Kxx) + \n",
    "            ((1/float(M*M))*Kyy) -\n",
    "            ((2/float(N*M))*Kxy)\n",
    "        ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     tqdm.write('Seizure kernel')\n",
    "    \n",
    "# #     seiz_Kxx = sklearn.metrics.pairwise.rbf_kernel(seizures, seizures)\n",
    "# #     seiz_Kxx = pairwise_kernels(seizures, seizures, metric='rbf')\n",
    "    \n",
    "# #     seiz_sum1 = np.sum(seiz_Kxx)/(len(seizures)*len(seizures))\n",
    "# #     seiz_sum2 = 0\n",
    "\n",
    "#     tqdm.write('Non-seizure kernel')\n",
    "#     nseiz_Kxx = 0\n",
    "    \n",
    "#     batches = np.array_split(non_seizures, BATCH_SIZE)\n",
    "#     print(len(batches))\n",
    "#     result = 0\n",
    "    \n",
    "# #     for batch in tqdm(batches):\n",
    "# #         rbf_kernel(non_seizures, batch)\n",
    "    \n",
    "#     with Pool(4) as p:\n",
    "#         result = list(tqdm(p.imap_unordered(partial(rbf_kernel, X=non_seizures), batches), total=len(batches)))\n",
    "#         p.close()\n",
    "#         p.join()\n",
    "        \n",
    "#     print(len(result))\n",
    "#     return None\n",
    "    \n",
    "# #     for data in tqdm(non_seizures):\n",
    "# #         dot_prods = sklearn.metrics.pairwise.rbf_kernel(data.reshape(1, -1), non_seizures)\n",
    "# #         nseiz_Kxx += np.sum(dot_prods)\n",
    "    \n",
    "#     nseiz_sum1 = np.sum(nseiz_Kxx)/(len(non_seizures)*len(non_seizures))\n",
    "#     nseiz_sum2 = 0\n",
    "    \n",
    "#     # slide a window over subject data\n",
    "#     for idx in tqdm(range(len(sub_data) - WIN_SIZE + 1), desc='Window loop'):\n",
    "\n",
    "#         window_data  = sub_data[idx:idx+WIN_SIZE-1, :]\n",
    "#         window_label = any(sub_labels[idx:idx+WIN_SIZE-1])\n",
    "#         truth.append(window_label)\n",
    "#         Kyy = pairwise_kernels(window_data, window_data, metric='rbf')\n",
    "#         seiz_Kxy  = pairwise_kernels(seizures, window_data, metric='rbf')\n",
    "#         nseiz_Kxy = pairwise_kernels(non_seizures, window_data, metric='rbf')\n",
    "        \n",
    "#         seiz_sum2  = np.sum(seiz_Kxy)*2/(len(seizures)*len(window_data))\n",
    "#         sseiz_sum2 = np.sum(nseiz_Kxy)*2/(len(non_seizures)*len(window_data))\n",
    "        \n",
    "#         sum3 = np.sum(Kyy)/(len(window_data)*len(window_data))\n",
    "        \n",
    "#         seiz_score  = seiz_sum1  - seiz_sum2  + sum3\n",
    "#         nseiz_score = nseiz_sum1 - nseiz_sum2 + sum3\n",
    "\n",
    "#         preds.append(seiz_score > nseiz_score)\n",
    "     \n",
    "    true_negative, true_positives = computeTrueNegAndPos(preds, truth)\n",
    "    \n",
    "    return true_negative, true_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussianKernel(x, y):\n",
    "    \"\"\"\n",
    "    Note: Also known as a radial basis function.\n",
    "          Kernel 3 from kernel functions link below.\n",
    "    \n",
    "    Parameters:\n",
    "        x - 1D vector of length N\n",
    "        y - 1D vector of length N\n",
    "    \n",
    "    Returns:\n",
    "        The value of the kernel function, i.e. a scalar\n",
    "    \"\"\"\n",
    "    sql2norm = np.sum(np.square(x - y))\n",
    "    sigma = 2\n",
    "    coeff = -1/2*(sigma*sigma)\n",
    "    \n",
    "    return np.exp(sql2norm * coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData():\n",
    "    \n",
    "    \"\"\"\n",
    "    Plot probability of seizure (and with non-seizure later) over time using\n",
    "    the best model found during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    directory = './probs/'\n",
    "    filename = '{}model.npy'.format(best[0])\n",
    "    file_path = directory + filename\n",
    "\n",
    "    probs = np.load(file_path)\n",
    "\n",
    "    print('Building plot of best model...')\n",
    "\n",
    "    time = np.arange(1, len(probs) + 1)\n",
    "    #val_neg = probs[ : , 0]\n",
    "    val_pos = probs[ : , 1]\n",
    "\n",
    "    # building plot\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    ax1.plot(time, val_pos, \"s-\")\n",
    "    ax1.set_ylabel(\"Probability of Seizure\")\n",
    "    ax1.set_xlabel(\"Current 4-second Time Interval\")\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.set_title('Probability of Seizure Over Time')\n",
    "    \n",
    "    print('Showing results for the best model:', best[0], '(leaving out non-seizures for now)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTable(tableData):\n",
    "    print('| Subject ID | True Negative Rate | True Positive Rate |')\n",
    "    print('--------------------------------------------------------')\n",
    "    \n",
    "    for data in tableData:\n",
    "        print('|', data[0], \" \" * (9 - len(data[0])), \n",
    "              '|', '{:.5f}'.format(data[1]), \" \" * (10), \n",
    "              '|', '{:.5f}'.format(data[2]), \" \" * (10), '|')\n",
    "    print('--------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to Know\n",
    "\n",
    "1) Data to train model is saved in the \"src/data/\" folder.\n",
    "\n",
    "2) Probabilities for each subject is saved in the \"src/probs/\" folder.\n",
    "\n",
    "3) I have not plotted seizure and non-seizure data against each other due to noise. This will get done once I confirm the correctness of the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "1) Finish plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels\n",
    "\n",
    "List of kernels for MMD: http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar(y,x):\n",
    "    t = x @ y.T\n",
    "    print('done')\n",
    "    return t\n",
    "\n",
    "def foo():\n",
    "    mat1 = np.ones((1000000, 14))\n",
    "    test = (np.ones((1,14)), np.ones((1,14)))\n",
    "\n",
    "    with Pool(8) as p:\n",
    "    # this will not finish\n",
    "        result = p.map(partial(bar, x=mat1), test)\n",
    "        p.close()\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features.npy...\n",
      "File was not found. Rebuilding data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'integrate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5e220f649be9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msubjectID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'chb01'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubjects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m#uniqueSubs = ['chb01', 'chb02']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0muniqueSubs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubjects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-c08105403ff6>\u001b[0m in \u001b[0;36mreadData\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m# TODO: compute the mean and variance of both channels of data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m# hint: use np.mean and np.var and look up the \"axis\" argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mband_relpower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_band_relpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;31m#logging.debug(\"readData() - band_relpower.shape: {}\".format(band_relpower.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m#logging.debug(\"readData() - type(band_relpower): {}\".format(type(band_relpower)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b6a94717b9d7>\u001b[0m in \u001b[0;36mcompute_band_relpower\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     12\u001b[0m     abs_power = np.concatenate(\n\u001b[1;32m     13\u001b[0m             [integrate.simps(psd[where(lb, ub),:], dx=fr_res, axis=0\n\u001b[0;32m---> 14\u001b[0;31m                 ).reshape(1,-1) for lb, ub in bands]\n\u001b[0m\u001b[1;32m     15\u001b[0m         )\n\u001b[1;32m     16\u001b[0m     \u001b[0mtotal_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintegrate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfr_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b6a94717b9d7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m     abs_power = np.concatenate(\n\u001b[1;32m     13\u001b[0m             [integrate.simps(psd[where(lb, ub),:], dx=fr_res, axis=0\n\u001b[0;32m---> 14\u001b[0;31m                 ).reshape(1,-1) for lb, ub in bands]\n\u001b[0m\u001b[1;32m     15\u001b[0m         )\n\u001b[1;32m     16\u001b[0m     \u001b[0mtotal_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintegrate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfr_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'integrate' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    subjectID = 'chb01'\n",
    "    features, labels, subjects = readData()\n",
    "    #uniqueSubs = ['chb01', 'chb02']\n",
    "    uniqueSubs = set(subjects)\n",
    "    tableData = []\n",
    "    best = None\n",
    "    foo()\n",
    "    \n",
    "    #print(features.shape)\n",
    "\n",
    "    #print('Running model...')\n",
    "    for sub in uniqueSubs:\n",
    "        #true_neg, true_pos = testModelOnSubject(sub, features, labels, subjects)\n",
    "        true_neg, true_pos = mmd(sub, features, labels, subjects)\n",
    "\n",
    "        if debug:\n",
    "            print('main - sub {}, true_neg {}, true_pos {}'.format(sub, true_neg, true_pos))\n",
    "            print([sub, true_neg, true_pos])\n",
    "\n",
    "        currSubResults = [sub, true_neg, true_pos]\n",
    "\n",
    "        if best == None or true_pos > best[2]:\n",
    "            best = currSubResults\n",
    "\n",
    "        tableData.append(currSubResults)\n",
    "    \n",
    "    print('Done! Results are shown below.\\n')  \n",
    "    printTable(tableData)\n",
    "    \n",
    "\n",
    "    plotData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import logging\n",
    "import os\n",
    "from scipy import integrate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import skew, kurtosis\n",
    "import itertools\n",
    "\n",
    "LOG = logging.getLogger(os.path.basename(''))\n",
    "\n",
    "SF = 256\n",
    "SEG_LENGTH = 512\n",
    "BANDS = [(1,4), (4,8), (8,12), (12,30)]\n",
    "EMBDEM = 7\n",
    "DELAY  = 1\n",
    "\n",
    "def main():\n",
    "    kernel_name = \"rbf\"\n",
    "#     if len(sys.argv) > 1:\n",
    "#         kernel_name = sys.argv[1]\n",
    "\n",
    "    if kernel_name == \"rbf\":\n",
    "        kernel = rbf_kernel\n",
    "    elif kernel_name == \"laplacian\":\n",
    "        kernel = laplacian_kernel\n",
    "    elif kernel_name == \"linear\":\n",
    "        kernel = linear_kernel\n",
    "    elif kernel_name == \"polynomial\":\n",
    "        kernel = polynomial_kernel\n",
    "    else:\n",
    "        raise NotImplementedError(\"Kernel: {} is invalid\".format(kernel_name))\n",
    "    \n",
    "    LOG.info(\"Using kernel: {}\".format(kernel_name))\n",
    "    \n",
    "    dirname = \"../temp/psd/{}\".format(kernel_name)\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    with tables.open_file(\"../input/eeg_data_temples2.h5\") as h5_file:\n",
    "        for node in h5_file.walk_nodes(\"/\", \"CArray\"):\n",
    "            LOG.info(\"Processing: {}\".format(node._v_name))\n",
    "            if len(node.attrs.seizures) != 1:\n",
    "                continue\n",
    "            \n",
    "            data = node.read()\n",
    "            seizures = node.attrs.seizures\n",
    "            X, y = data[:,:-1], data[:,-1]\n",
    "            start = np.min(np.where(y > 0)[0])\n",
    "            stop = np.max(np.where(y > 0)[0])\n",
    "\n",
    "            buff_mins = 20\n",
    "            minv = max(0, start-(buff_mins*60*SF))\n",
    "            maxv = min(X.shape[0], stop+(buff_mins*60*SF))\n",
    "            data = data[minv:maxv,:]\n",
    "            X = X[minv:maxv,:]\n",
    "            y = y[minv:maxv]\n",
    "            \n",
    "            sos = signal.butter(3, 50, fs=SF, btype=\"lowpass\", output=\"sos\")\n",
    "            X = signal.sosfilt(sos, X, axis=1)\n",
    "            Z = []\n",
    "            q = []\n",
    "            moments   = []\n",
    "            \n",
    "            for ix in range(SEG_LENGTH, X.shape[0], SEG_LENGTH):\n",
    "                \n",
    "                segment   = X[ix-SEG_LENGTH:ix,:]\n",
    "                ord_freqs = ordinal_patterns(segment, EMBDEM, DELAY)\n",
    "                \n",
    "                means         = np.mean(segment, axis=0) # append mean\n",
    "                variances     = np.var(segment, axis=0)  # append variance \n",
    "                skew_         = skew(segment, axis=0)\n",
    "                kurtosis_     = kurtosis(segment, axis=0)\n",
    "                perm_entro    = s_entropy(ord_freqs, EMBDEM)/np.log(math.factorial(EMBDEM))\n",
    "                print(perm_entro)\n",
    "                band_relpower = compute_band_relpower(segment)\n",
    "                \n",
    "                moments.append(np.concatenate((means, variances, skew_, kurtosis_)))\n",
    "                Z.append((band_relpower, perm_entro))\n",
    "                q.append(np.any(y[ix-SEG_LENGTH:ix]))\n",
    "            \n",
    "            moments = np.vstack(moments)\n",
    "            scaler = MinMaxScaler()\n",
    "            moments = scaler.fit_transform(moments)\n",
    "            \n",
    "            Z = np.vstack(Z)\n",
    "            Z = np.hstack((Z, moments))\n",
    "            y = np.array(q)\n",
    "            \n",
    "            #ignore below\n",
    "            \n",
    "#             band_names = [\"{}-{}\".format(x,y) for x,y in BANDS]\n",
    "#             colnames = [(x + \"_1\", x + \"_2\") for x in band_names]\n",
    "        \n",
    "#             bands = pd.DataFrame(Z, columns=[n for x in colnames for n in x])\n",
    "#             plt.close()\n",
    "#             bands.plot()\n",
    "#             plt.axvline(x=np.min(np.where(y > 0)[0]), linewidth=2, color=\"red\")\n",
    "#             plt.axvline(x=np.max(np.where(y > 0)[0]), linewidth=2, color=\"red\")            \n",
    "#             plt.legend()\n",
    "#             plt.savefig(\"{}/{}_signal_relpower.png\".format(dirname, node._v_name))\n",
    "\n",
    "            print(Z[0])\n",
    "            K = kernel(Z)\n",
    "            mmd = []\n",
    "            for N in tqdm(range(1,Z.shape[0])):\n",
    "                M = Z.shape[0] - N\n",
    "                Kxx = K[:N,:N].sum()\n",
    "                Kxy = K[:N,N:].sum()\n",
    "                Kyy = K[N:,N:].sum()\n",
    "                mmd.append(np.sqrt(\n",
    "                    ((1/float(N*N))*Kxx) + \n",
    "                    ((1/float(M*M))*Kyy) -\n",
    "                    ((2/float(N*M))*Kxy)\n",
    "                ))\n",
    "            \n",
    "            ws = []\n",
    "            mmd = np.array(mmd)\n",
    "            mmd_corr = np.zeros(mmd.size)\n",
    "            for ix in range(1,mmd_corr.size):\n",
    "                w = ((Z.shape[0]-1) / float(ix*(N-ix)))\n",
    "                ws.append(w)\n",
    "                mmd_corr[ix] = mmd[ix] - w*mmd.max()\n",
    "\n",
    "            # mmd = mmd[200:-200]\n",
    "            # mmd_corr = mmd_corr[200:-200]\n",
    "            plt.close()\n",
    "            plt.plot(mmd, label=\"MMD\")\n",
    "            plt.plot(mmd_corr, label=\"MMD (Corrected)\")\n",
    "            plt.axvline(x=np.min(np.where(y > 0)[0]), linewidth=2, color=\"red\")\n",
    "            plt.axvline(x=np.max(np.where(y > 0)[0]), linewidth=2, color=\"red\")\n",
    "            plt.savefig(\"{}/{}_mmd.png\".format(dirname, node._v_name))\n",
    "\n",
    "\n",
    "def ordinal_patterns(ts, embdim, embdelay):\n",
    "    ''' Computes the ordinal patterns of a time series for a given embedding dimension and embedding delay.\n",
    "    USAGE: ordinal_patterns(ts, embdim, embdelay)\n",
    "    ARGS: ts = Numeric vector represnting the time series, embdim = embedding dimension (3<=embdim<=7 prefered range), embdelay =  embdding delay\n",
    "    OUPTUT: A numeric vector representing frequencies of ordinal patterns'''\n",
    "    time_series = ts\n",
    "    possible_permutations = list(itertools.permutations(range(embdim)))\n",
    "    lst = list()\n",
    "    \n",
    "    # Number of vectors is T - t(D - 1)\n",
    "    for i in range(len(time_series) - embdelay * (embdim - 1)):\n",
    "        sorted_index_array = list(np.argsort(time_series[i:(embdim+i)]))\n",
    "        lst.append(sorted_index_array)\n",
    "        \n",
    "    lst = np.array(lst)\n",
    "    element, freq = np.unique(lst, return_counts = True, axis = 0)\n",
    "    freq = list(freq)\n",
    "    \n",
    "    # Fix missing entries\n",
    "    if len(freq) != len(possible_permutations):\n",
    "        for i in range(len(possible_permutations)-len(freq)):\n",
    "            freq.append(0)\n",
    "    freq = [x/len(possible_permutations) for x in freq]\n",
    "    return(freq)\n",
    "    \n",
    "def s_entropy(freq_list, d):\n",
    "    ''' This function computes the shannon entropy of a given frequency distribution.\n",
    "    USAGE: shannon_entropy(freq_list)\n",
    "    ARGS: freq_list = Numeric vector represnting the frequency distribution\n",
    "    OUTPUT: A numeric value representing shannon's entropy'''\n",
    "    freq_list = [element for element in freq_list if element != 0]\n",
    "    sh_entropy = 0.0\n",
    "    \n",
    "    for freq in freq_list:\n",
    "        sh_entropy += freq * np.log(freq)\n",
    "        \n",
    "    sh_entropy = -sh_entropy\n",
    "    \n",
    "    return(sh_entropy)\n",
    "            \n",
    "def compute_band_relpower(X):\n",
    "    freqs, psd = signal.welch(X, SF, axis=0)\n",
    "    freq_res = freqs[1] - freqs[0]\n",
    "    total_power = integrate.simps(psd, dx=freq_res, axis=0)\n",
    "\n",
    "    where = total_power <= 1e-5\n",
    "    total_power[where] = -1\n",
    "\n",
    "    band_relpower = []\n",
    "    for lb, ub in BANDS:\n",
    "        idx = np.logical_and(freqs >= lb, freqs < ub)\n",
    "        band_power = integrate.simps(psd[idx,:], dx=freq_res, axis=0)\n",
    "        relpow = band_power / total_power\n",
    "        relpow[where] = 0\n",
    "        band_relpower.append(relpow)\n",
    "    \n",
    "    return np.concatenate(band_relpower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04940126258648394\n",
      "0.051105884709514854\n",
      "0.04725965963920595\n",
      "0.04925153876467387\n",
      "0.048885712394975724\n",
      "0.04920862170825991\n",
      "0.04840930755280153\n",
      "0.048737559552424335\n",
      "0.052454320630366764\n",
      "0.05065589679939838\n",
      "0.047900306854777745\n",
      "0.04876846609367425\n",
      "0.05466020304237673\n",
      "0.05378486390232217\n",
      "0.05521778997402952\n",
      "0.04855792419509559\n",
      "0.05289987933856064\n",
      "0.049478882120768515\n",
      "0.05437252548343754\n",
      "0.04788617691245361\n",
      "0.04874162568531177\n",
      "0.047281157050129904\n",
      "0.04689943564011483\n",
      "0.043541052913423944\n",
      "0.052137189720396625\n",
      "0.0489409288328108\n",
      "0.049410903831507605\n",
      "0.04527204895670539\n",
      "0.04671934739633056\n",
      "0.05104109050595784\n",
      "0.04819131934626084\n",
      "0.04947491759578718\n",
      "0.05042206563554061\n",
      "0.047529732555389795\n",
      "0.05241036819153681\n",
      "0.050624838062214475\n",
      "0.05039268475819943\n",
      "0.05662403404606763\n",
      "0.04771383246076432\n",
      "0.04971593905788431\n",
      "0.05415896003954603\n",
      "0.05742296789673946\n",
      "0.04781165720461009\n",
      "0.04919705445313109\n",
      "0.05297123044493927\n",
      "0.047394957688755825\n",
      "0.05007562183751326\n",
      "0.05479426294186736\n",
      "0.04880251401010303\n",
      "0.049928263905079526\n",
      "0.058934683728633405\n",
      "0.048204133886193705\n",
      "0.0504451934342418\n",
      "0.05123564648231316\n",
      "0.04962527308688929\n",
      "0.05189868816045506\n",
      "0.05678796188338802\n",
      "0.047746945687130025\n",
      "0.052372941806415545\n",
      "0.049666598345445095\n",
      "0.05100332352457745\n",
      "0.04772843886386077\n",
      "0.0494985928752892\n",
      "0.052431637711708\n",
      "0.052159522995714865\n",
      "0.05206524583216847\n",
      "0.05190642722450232\n",
      "0.04957360168721493\n",
      "0.05242365804227516\n",
      "0.05129259286563676\n",
      "0.05227538023762158\n",
      "0.05196635731570593\n",
      "0.048768691983819765\n",
      "0.050366399184098645\n",
      "0.05178235306625643\n",
      "0.05316421818587971\n",
      "0.05349655991455882\n",
      "0.05516279812952351\n",
      "0.054434273396304286\n",
      "0.04993816120808795\n",
      "0.053191352688957\n",
      "0.049122199705622226\n",
      "0.04617118120940097\n",
      "0.04835912548114071\n",
      "0.050481551037997875\n",
      "0.050309880054917516\n",
      "0.05292378357626051\n",
      "0.05036589679230244\n",
      "0.050550035108741814\n",
      "0.05120733640517519\n",
      "0.05329330355334188\n",
      "0.05066236220816986\n",
      "0.047527726870220055\n",
      "0.04980818493124285\n",
      "0.049211322797145296\n",
      "0.050839525657299024\n",
      "0.05433433868399146\n",
      "0.050136640005594965\n",
      "0.05615245171666757\n",
      "0.04795558443403842\n",
      "0.05011692718250274\n",
      "0.04791125370879431\n",
      "0.055625309812968766\n",
      "0.05517786185116458\n",
      "0.05470811321385101\n",
      "0.05372287802062168\n",
      "0.0499968103948627\n",
      "0.051113112092938826\n",
      "0.05177758222074015\n",
      "0.05708043214039978\n",
      "0.054618903420983654\n",
      "0.051085008440994006\n",
      "0.050712716989013806\n",
      "0.05433236009568562\n",
      "0.05104292140685693\n",
      "0.05151628640244992\n",
      "0.04896734706640116\n",
      "0.04659918610560449\n",
      "0.05245075625834587\n",
      "0.05267694329299314\n",
      "0.0544777286598986\n",
      "0.05049433598551548\n",
      "0.05555164793419937\n",
      "0.04974591176825496\n",
      "0.05290539502077046\n",
      "0.053788921120763\n",
      "0.05171785984024716\n",
      "0.050987445747369256\n",
      "0.048792488528568684\n",
      "0.04985786609614348\n",
      "0.04988301037559097\n",
      "0.052197993151158494\n",
      "0.04679641247470884\n",
      "0.04838532797119137\n",
      "0.05442623014645248\n",
      "0.04711384685171722\n",
      "0.04877947199930445\n",
      "0.048860439941087075\n",
      "0.05089218013138623\n",
      "0.04881313567819885\n",
      "0.05006847388322962\n",
      "0.052540600655358274\n",
      "0.04830663691817151\n",
      "0.048061591730173284\n",
      "0.051103709464542006\n",
      "0.051344367710535584\n",
      "0.04957476731168593\n",
      "0.05044915202639939\n",
      "0.05405273545203476\n",
      "0.05504575512148027\n",
      "0.04692426618681724\n",
      "0.04736913783938771\n",
      "0.05003002787742043\n",
      "0.055127322867534655\n",
      "0.04430687343062691\n",
      "0.049999108127885404\n",
      "0.051993795738667185\n",
      "0.048842770621782064\n",
      "0.049567036762628566\n",
      "0.055626057217016155\n",
      "0.0504947194401897\n",
      "0.05276879372834381\n",
      "0.056993816165557126\n",
      "0.05350384105857833\n",
      "0.05270732235844784\n",
      "0.049722537482530554\n",
      "0.04825025132595765\n",
      "0.04611396070921425\n",
      "0.04829469174685824\n",
      "0.047543035337542415\n",
      "0.048614416467160304\n",
      "0.04903330584396429\n",
      "0.05388173836232563\n",
      "0.05214825635364695\n",
      "0.04741608964321032\n",
      "0.052931967510378305\n",
      "0.05365415652076926\n",
      "0.053030592653900824\n",
      "0.04754701039536417\n",
      "0.05178484671627117\n",
      "0.05711413087732305\n",
      "0.05057252687197758\n",
      "0.04660799413086112\n",
      "0.04870640562209504\n",
      "0.0493261876313527\n",
      "0.054519702922971994\n",
      "0.05024999968218141\n",
      "0.05353425706143278\n",
      "0.04858489627695362\n",
      "0.05226949027451989\n",
      "0.04825922015957366\n",
      "0.04857651269779834\n",
      "0.05344653913882176\n",
      "0.04953745178046654\n",
      "0.05059936988590258\n",
      "0.05918616518616493\n",
      "0.05162699688361333\n",
      "0.052293195775175674\n",
      "0.050302478595654086\n",
      "0.047156790230684316\n",
      "0.046635878129340363\n",
      "0.05325452134446153\n",
      "0.04645069317910088\n",
      "0.05579683190076278\n",
      "0.05592201904506442\n",
      "0.047599093291266756\n",
      "0.05322799174080041\n",
      "0.0544737419082559\n",
      "0.05542710054493725\n",
      "0.05760816385009826\n",
      "0.053239735156975346\n",
      "0.04524033716573129\n",
      "0.045666143153963834\n",
      "0.052223731729662666\n",
      "0.049803655255004325\n",
      "0.04892721251837102\n",
      "0.05100941574752068\n",
      "0.05458149914288187\n",
      "0.05150840079382368\n",
      "0.0521151854540149\n",
      "0.046072997167601173\n",
      "0.062159868938300306\n",
      "0.0504833473166108\n",
      "0.050435623626647955\n",
      "0.04633680568712139\n",
      "0.049417049357909505\n",
      "0.0501485155256305\n",
      "0.05796100562872362\n",
      "0.056108652376951375\n",
      "0.045093212894573693\n",
      "0.04975544716484444\n",
      "0.048421925754623174\n",
      "0.05355900038212539\n",
      "0.04762112024608872\n",
      "0.047744394562038016\n",
      "0.04745532364661618\n",
      "0.04890557039190451\n",
      "0.048769336682962426\n",
      "0.05038763284724081\n",
      "0.047631567185432884\n",
      "0.04993389215316462\n",
      "0.05389520915339727\n",
      "0.05021733426425892\n",
      "0.05295805494548086\n",
      "0.051510296609324456\n",
      "0.04779145879192243\n",
      "0.053381051398554605\n",
      "0.05028309587548699\n",
      "0.05272318142216427\n",
      "0.05492039794505199\n",
      "0.05650494224157537\n",
      "0.04999686205540249\n",
      "0.05506595321464989\n",
      "0.05654360016874164\n",
      "0.05368633362545007\n",
      "0.060370566514855055\n",
      "0.0539358250703728\n",
      "0.0549168954885694\n",
      "0.0588351905998206\n",
      "0.057611796677703234\n",
      "0.053719065757839565\n",
      "0.051854436870360676\n",
      "0.049364592898223425\n",
      "0.0586040602554427\n",
      "0.05507737016930514\n",
      "0.04754784922519325\n",
      "0.05550100048183703\n",
      "0.04980858145523219\n",
      "0.052961823608667635\n",
      "0.04907917429844872\n",
      "0.058392839852504104\n",
      "0.05613122696970446\n",
      "0.05360273145733295\n",
      "0.05321575612874659\n",
      "0.05164966917716439\n",
      "0.05718753706252705\n",
      "0.054694738680904444\n",
      "0.04937511103487947\n",
      "0.05093275850653742\n",
      "0.053027096183113194\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-af1dd977f9b0>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0msegment\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mSEG_LENGTH\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m                 \u001b[0mord_freqs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mordinal_patterns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msegment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEMBDEM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDELAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[0mmeans\u001b[0m         \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msegment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# append mean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-af1dd977f9b0>\u001b[0m in \u001b[0;36mordinal_patterns\u001b[1;34m(ts, embdim, embdelay)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;31m# Number of vectors is T - t(D - 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_series\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0membdelay\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0membdim\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0msorted_index_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_series\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membdim\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[0mlst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted_index_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margsort\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margsort\u001b[1;34m(a, axis, kind, order)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m     \"\"\"\n\u001b[1;32m-> 1105\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argsort'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
